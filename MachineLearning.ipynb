{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1 training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z = w^Tx + b \\\\\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "L(\\hat{y}, y) = -(y\\log\\hat{y} + (1 - y)\\log(1 - \\hat{y}))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function: Multiclass log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "L(\\hat{y}, y) = -(y\\log\\hat{y} + (1 - y)\\log(1 - \\hat{y}))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "y = 1 \\\\\n",
    "\\hat{y} = 0.99\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "L(\\hat{y}, y) = -(1\\log{0.99} + (1 - 1)\\log(1 - 0.99)) \\\\\n",
    "L(\\hat{y}, y) = -1\\log{0.99} \\\\\n",
    " = 0.004364\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "y = 1 \\\\\n",
    "\\hat{y} = 0.01\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "L(\\hat{y}, y) = -(1\\log{0.01} + (1 - 1)\\log(1 - 0.01)) \\\\\n",
    "L(\\hat{y}, y) = -1\\log{0.01} \\\\\n",
    " = 2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $y = 1$, $L(\\hat{y}, y) = -\\log\\hat{y}$, we need $\\hat{y}$ to be as large as possible so that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "y = 0 \\\\\n",
    "\\hat{y} = 0.99\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "L(\\hat{y}, y) = -(0\\log{0.99} + (1 - 0)\\log(1 - 0.99)) \\\\\n",
    "L(\\hat{y}, y) = -1\\log{0.01} \\\\\n",
    " = 2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "y = 0 \\\\\n",
    "\\hat{y} = 0.01\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "L(\\hat{y}, y) = -(0\\log{0.01} + (1 - 0)\\log(1 - 0.01)) \\\\\n",
    "L(\\hat{y}, y) = -1\\log{0.99} \\\\\n",
    " = 0.004364\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $y = 0$, $L(\\hat{y}, y) = -\\log(1 - \\hat{y})$, we need $\\hat{y}$ to be as small as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $m$ training examples, add them up and average them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "J(\\hat{y}, y) = -\\frac{1}{m}\\sum_{i = 0}^{m - 1}(y^{(i)}\\log\\hat{y}^{(i)} + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)})) \n",
    "\\end{align*}\n",
    "Where data is indexed from $0$ as in most programming languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, for a single training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z = w^Tx + b\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that for this one training examples, there are $2$ features $x_1$ and $x_2$, the reason for assuming $2$ features is so that, we can extend this example to multiple features. The goal is to explain how gradient descent update takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z = w_1x_1 + w_2x_2 + b \\\\\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "L(\\hat{y}, y) = -(y\\log\\hat{y} + (1 - y)\\log(1 - \\hat{y}))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For notational convenience, Let $L(\\hat{y}, y)$ be represented by $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Update Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient descent, we perform the following update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "w_1 = w_1 - \\alpha\\frac{\\partial L}{\\partial w_1} \\\\\n",
    "w_2 = w_2 - \\alpha\\frac{\\partial L}{\\partial w_2} \\\\\n",
    "b = b -  \\alpha\\frac{\\partial L}{\\partial b} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would represent ONE iteration of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an updated $w_1$, $w_2$, we refit them in our equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z = w_1x_1 + w_2x_2 + b \\\\\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "L(\\hat{y}, y) = -(y\\log\\hat{y} + (1 - y)\\log(1 - \\hat{y}))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then, do the performance update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "w_1 = w_1 - \\alpha\\frac{\\partial L}{\\partial w_1} \\\\\n",
    "w_2 = w_2 - \\alpha\\frac{\\partial L}{\\partial w_2} \\\\\n",
    "b = b -  \\alpha\\frac{\\partial L}{\\partial b} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for many iterations and finally end up with $w_1$, $w_2$ and $b$ which we would go on to use for our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways we can think about how to perform this gradient descent update. Rewriting our equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z = w_1x_1 + w_2x_2 + b \\\\\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "L = -(y\\log\\hat{y} + (1 - y)\\log(1 - \\hat{y})) \\\\\n",
    "L = -(y\\log\\sigma(z) + (1 - y)\\log(1 - \\sigma(z))) \\\\\n",
    "L = -\\left(y\\log\\frac{1}{1 + e^{-z}} + (1 - y)\\log\\left(1 - \\frac{1}{1 + e^{-z}}\\right)\\right) \\\\\n",
    "L = -\\left(y\\log\\frac{1}{1 + e^{-(w_1x_1 + w_2x_2 + b)}} + (1 - y)\\log\\left(1 - \\frac{1}{1 + e^{-(w_1x_1 + w_2x_2 + b)}}\\right)\\right) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then take it from here. Or you could simply use the chain rule! But first, let's get a small intuition of the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a race, Usain Bolt is travelling twice as fast as a train which is going 3 times as fast as a horse. How do I represent Usain Bolt's speed with respect to the horse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{Bolt}}{\\partial\\text{Horse}}= \\frac{\\partial\\text{Bolt}}{\\partial\\text{Train}} \\cdot \\frac{\\partial\\text{Train}}{\\partial\\text{Horse}} = 2\\cdot 3 = 6\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z = w_1x_1 + w_2x_2 + b \\\\\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "L = -(y\\log\\hat{y} + (1 - y)\\log(1 - \\hat{y}))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to find, $\\frac{\\partial L}{\\partial w_1}$, we need to know how $L$ moves with respect to $\\hat{y}$ whose movement is affected by how $z$ moves, which in turn is affected by $w_1$, $w_2$ and $b$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial w_1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} \\\\\n",
    "\\frac{\\partial \\hat{y}}{\\partial z} = \\frac{\\partial \\sigma (z)}{\\partial z} = \\sigma (z)\\cdot (1-\\sigma(z)) = \\hat{y}\\cdot(1 - \\hat{y}) \\\\\n",
    "\\frac{\\partial z}{\\partial w_1} = x_1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_1} = x_1\\cdot(\\hat{y} - y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you can see a pattern emerging here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_2} = x_2\\cdot(\\hat{y} - y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b} = \\hat{y} - y\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Update for a single iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J = 0$, $\\frac{\\partial L}{\\partial w_1} = 0$, $\\frac{\\partial L}{\\partial w_2} = 0$, $\\frac{\\partial L}{\\partial b} = 0$, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $m$ training examples, we would do the following for 1 iteration of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $i = 0$ to $i = m$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z^{(i)} = w^Tx^{(i)} + b \\\\\n",
    "\\hat{y}^{(i)} = \\sigma(z^{(i)}) \\\\\n",
    "J = -(y^{(i)}\\log\\hat{y}^{(i)} + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)})) \\\\\n",
    " \\frac{\\partial L}{\\partial z}^{(i)} = \\hat{y}^{(i)} - y^{(i)} \\\\\n",
    " \\frac{\\partial L}{\\partial w_1} =  \\frac{\\partial L}{\\partial w_1} + x_1^{(i)}(\\hat{y}^{(i)} - y^{(i)}) \\\\\n",
    " \\frac{\\partial L}{\\partial w_2} =  \\frac{\\partial L}{\\partial w_2} + x_2^{(i)}(\\hat{y}^{(i)} - y^{(i)}) \\\\\n",
    " \\frac{\\partial L}{\\partial b} =  \\frac{\\partial L}{\\partial b} + (\\hat{y}^{(i)} - y^{(i)})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J = \\frac{J}{m}$, $\\frac{\\partial L}{\\partial w_1} = \\frac{\\frac{\\partial L}{\\partial w_1}}{m}$, $\\frac{\\partial L}{\\partial w_2} = \\frac{\\frac{\\partial L}{\\partial w_2}}{m}$, $\\frac{\\partial L}{\\partial b} = \\frac{\\frac{\\partial L}{\\partial b}}{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding $\\frac{\\partial L}{\\partial w_1}$, $\\frac{\\partial L}{\\partial w_2}$ and $\\frac{\\partial L}{\\partial b}$  make the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "w_1 = w_1 - \\alpha\\frac{\\partial L}{\\partial w_1} \\\\\n",
    "w_2 = w_2 - \\alpha\\frac{\\partial L}{\\partial w_2} \\\\\n",
    "b = b -  \\alpha\\frac{\\partial L}{\\partial b} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Vectorized Logistic Regression for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Attendance (x1)  Math score (x2)  True label (y)\n",
      "0               50               54               0\n",
      "1               60               70               1\n",
      "2               80               90               1\n",
      "3               90               99               1\n",
      "4               21               22               0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame([[50, 60, 80, 90, 21], [54, 70, 90, 99, 22], [0, 1, 1, 1, 0]]).T\n",
    "df.columns = ['Attendance (x1)', 'Math score (x2)', 'True label (y)']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cost after 0 iteration is 0.6879521123839162\n",
      "Average cost after 1 iteration is 0.6480712626560493\n",
      "Average cost after 2 iteration is 0.5600657119963963\n"
     ]
    }
   ],
   "source": [
    "w1 = 0.0001 #Initial values\n",
    "w2 = 0.0002 #Initial values\n",
    "b = 0 #Initial values\n",
    "\n",
    "def cost(y, yhat):\n",
    "    \n",
    "    return -((y*np.log(yhat)) + (1 - y)*np.log(1 - yhat))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-(x)))\n",
    "\n",
    "for iter_num in range(0, 3):\n",
    "    \n",
    "    \n",
    "    J = 0\n",
    "    dw1 = 0\n",
    "    dw2 = 0\n",
    "    db = 0\n",
    "\n",
    "\n",
    "    for row in df.index:\n",
    "\n",
    "        x1 = df.loc[row]['Attendance (x1)']\n",
    "        x2 = df.loc[row]['Math score (x2)']\n",
    "        y = df.loc[row]['True label (y)']\n",
    "        z = w1*x1 + w2*x2 + b\n",
    "        #print(\"z for training example no. {} is {}\".format(i, z))\n",
    "        yhat = sigmoid(z)\n",
    "        \n",
    "        #print(\"yhat (Prediction) for training example {} is {}\".format(i, yhat))\n",
    "        #print(\"y (True label) for training example {} is {}\".format(i, y))\n",
    "        J = J + cost(y, yhat)\n",
    "        #print(\"J (Cost) for training example {} is {}\".format(i, cost(y, yhat)))\n",
    "\n",
    "        dz = (yhat - y)\n",
    "        \n",
    "        \n",
    "        dw1 = dw1 + x1*(yhat - y)\n",
    "        dw2 = dw2 + x2*(yhat - y)\n",
    "        db = db + dz\n",
    "    \n",
    "\n",
    "    dw1 =  dw1/len(df)\n",
    "    dw2 =  dw2/len(df)\n",
    "    db = db/len(df)\n",
    "    \n",
    "    J = J/len(df)\n",
    "    print(\"Average cost after {} iteration is {}\".format(iter_num, J))\n",
    "    w1 = w1 - 0.001*dw1\n",
    "    w2 = w2 - 0.001*dw2\n",
    "    b = b - 0.001*db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007117425235320776 0.009698896322646137\n"
     ]
    }
   ],
   "source": [
    "print(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Logistic Regression for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a matrix out of our training data, and out of our labels. The training data that has the features and the samples must be set in such a way that each column is a training example, as opposed to how normally we assume by default that each row is a training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50 60 80 90 21]\n",
      " [54 70 90 99 22]]\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "data = df[['Attendance (x1)', 'Math score (x2)']]\n",
    "data = data.T\n",
    "\n",
    "data = np.array(data)\n",
    "print(data)\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Your label `(y)` must be a row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "[[0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "y = df[['True label (y)']]\n",
    "y = np.array(y.T)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Your weights `(w)` should be matrix of dimension `(num_features, 1)`. We know that `b` and `db` and `J` are scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "\n",
    "    w = np.array([0.0001, 0.0002])\n",
    "    w = w.reshape((w.shape[0], 1))\n",
    "    \n",
    "    b = 0\n",
    "    \n",
    "    \n",
    "    return w, b\n",
    "\n",
    "def cost(y, yhat):\n",
    "    \n",
    "    return -((y*np.log(yhat)) + (1 - y)*np.log(1 - yhat))\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    return 1/(1 + np.exp(-(x)))\n",
    "\n",
    "def update_param(w, b, dw, db):\n",
    "    \n",
    "    w = w - 0.001*dw\n",
    "    b = b - 0.001*db\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(w, b, num_iter, y, data, m):\n",
    "    \n",
    "    for iter_num in range(0, num_iter):\n",
    "        \n",
    "        z = np.dot(w.T, data) + b\n",
    "\n",
    "        yhat = sigmoid(z)\n",
    "\n",
    "        J = np.mean(cost(y, yhat))\n",
    "        print(\"Cost: \", J)\n",
    "\n",
    "        dz = (yhat - y)\n",
    "\n",
    "        dw = np.dot(data, dz.T)/m\n",
    "        db = np.mean(dz)\n",
    "\n",
    "        w, b = update_param(w, b, dw, db)\n",
    "        \n",
    "    return w, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0.6879521123839162\n",
      "Cost:  0.6480712626560493\n",
      "Cost:  0.5600657119963963\n"
     ]
    }
   ],
   "source": [
    "w, b = initialize()\n",
    "w, b = forwardprop(w, b, 3, y, data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00711743],\n",
       "       [0.0096989 ]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00030145999968170397"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of learning rate\n",
    "\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need for random initialization of weights\n",
    "\n",
    "In Logistic Regression, it is okay to initialize weights to zero. But for a shallow neural network or a deep neural network, the hidden units become completely symmetric and compute the exact some function (can be proven by induction), the hidden units are symmetric even after many iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks (Shallow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
